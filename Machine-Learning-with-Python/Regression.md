# Regression

## Linear Regression

### Introduction to Regression

what is regression ?
Regression is process of predicting a continuous value.

| ENGINESIZE | CYLINDERS | FUELCONSUMPTION_COMB | CO2EMISSIONS |
|--- |---|--- |---|
| 0 | 2.0 | 4 | 8.5 | 196 |
| 1 | 2.4 | 4 | 9.6 | 221 |
| 2 | 1.5 | 4 | 5.9 | 136 |
| 3 | 3.5 | 6 |11.1 | 255 |
| 4 | 3.5 | 6 |10.6 | 244 |
| 5 | 3.5 | 6 |10.0 | 230 |
| 6 | 3.5 | 6 |10.1 | 232 |
| 7 | 3.7 | 6 |11.1 | 255 |
| 8 | 3.7 | 6 |11.6 | 267 |
| 9 | 2.4 | 4 | 9.2 | ? | <- Let's predict co2emission for new car

In regression there are two types of variable: a dependent variable and one or more independent variables.

**Dependent variable:** The dependent variable can be seen as the state, target or final goal we study and try to predict. It is notated by Y.
**Independent variable:** It is also called as explanatory variables, can be seen as the causes of these states. It is notated by X.
A regression model relates Y to a function of X.
In above dataset, ENGINESIZE, CYLINDERS, FUELCONSUMPTION_COMB are dependent variable and CO@EMISSIONS is independent variable. 
In regression columns are called as features. The key point in regression is that our dependent value should be continuous and can not be a discrete value.
However the independent variable can be measured on either a categorical or continuous measurement scale.

Using above data, we are going to build regression estimation model, this model is going to predict the expected co2 emission for a new or unknown car.

There are two type of regression models: simple regression model and multiple regression.

**Simple Linear Regression:** One independent variable is used to estimate a dependent variable. It can be either linear or non linear. eg: predicting co2 emission using the variable of engine size. Linearity of regression is based on the nature of relationship between dependent and independent variable.

**Multiple Regression:** When more than one independent variable is present the process is called multiple linear regression. eg: predicting co2 emission using engine size and the number of cylinders in any given car.

Depending on the relationship between dependent or independent variables it can be linear or non-linear.

Applications of regression:
- Sales forecasting
- Satisfaction analysis
- Price estimation
- Employement income

### Simple Linear Regression:
Equation:
**y' = B0 + B1*x**

y' = response variable
x = a single protector

#### How to find the best fit?
Example:
x = 5.4 independent variable
y = 250 actual Co2 emission of x1
y' = 340 the predicted emission of x1
error = y-y'= 250-340 = -90

**MSE formula** = (1/n) * Σ(actual – forecast)2
Where:

    n = number of items,
    Σ = summation notation,
    Actual = original or observed y-value,
    Forecast = y-value from regression.

General steps to calculate the MSE from a set of X and Y values:
- Find the regression line.
- Insert your X values into the linear regression equation to find the new Y values (Y’).
- Subtract the new Y value from the original to get the error.
- Square the errors.
- Add up the errors (the Σ in the formula is summation notation).
- Find the mean.

### Pros of linear regression:
- Very fast.
- No parameter tuning.
- Easy to understand and highly interpretable.

### Model Evaluation in Regression Models:
- Train and Test on the Same Dataset.
- Train/Test Split.

### What is training & out-of-sample accuracy?
- Training Accuracy
    - High training accuracy isn't necessarily a good thing.
    - Result over-fitting: **Over-fit:** the model is overly trained to the dataset, which may capture noise and produce a non-generalized model.
- Out-of-Sample Accuracy:
    - It's important that our models have a high, out-of-sample accuracy.
    - How can we improve out-of-sample accuracy?
### How to use k-fold cross-validation ?
- 1st Fold - 80%(first 25% data used in testin, rest will be used in training the model)
- 2nd Fold - 84%(second 25% data which is not used in first)
- 3rd Fold - 82%(3rd 25% data which is not used in first,second)
- 4th Fold - 86%(4th 25% data which is not used in first,second, third)
Accuracy = avg(80%+84%+82%+86%) = 83%

### Evaluation Metrics in Regression Models:
Error: The difference between the data points and the trend line generated by the algorithm. Difference between Predicted and Actual Values.
Regression accuracy to determine the error: MAE, MSE, RMSE, RAE, RSE, R2
    - MAE(Mean Absolute Error): Mean of the absolute value of the error. In other words, Its just the average error.
    - MSE(Mean Squared Error): Mean of the squared errors. Focus on large errors.
    - RMSE(Root Mean Square Error): Square root of the mean squared error.
    - RAE(Relative Absolute Error): Also called Residual sum of square. It takes the total absolute error and normalize it by diving the total absolute error of the simple predictor.
    - RSE(Relative Squared Error): It is used to calculate R-squared.
    - R-squared: It is not an error but widely used metrics to check the accuracy of model. It reprsent how close the data values are to the fitted regression line.The higher the R-sqaured value, the better the model fits your data.


### Multiple Linear Regression
- Independent variables effectiveness on prediction
    - Does revision time, test anxiety, lecture attendance and gender have any effect on the exam performance of students ?
- Predicting impacts of changes
    - How much does blood pressure go up (or down) for every unit increase (or decrease) in the BMI of a patient?
- Multiple linear Regrssion is a method of predicting continous values.
    - y' = B0 + B1X1 + B2X2 + .... + BnXn
    - How to estimate B value ?
        - Oridnary least Sqaures: Use Linear algebara operations but take long time for large datasets(10K+ rows)
        - 
